kind: ConfigMap
apiVersion: v1
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "3"
  name: llama-stack-config
data:
  run.yaml: |
    version: '2'
    image_name: vllm
    apis:
    - agents
    - datasetio
    - eval
    - inference
    - safety
    - scoring
    - tool_runtime
    - vector_io
    - telemetry
    providers:
      inference:
      - provider_id: vllm
        provider_type: remote::vllm
        config:
          url: ${env.GRANITE_URL:http://granite-8b-predictor.llm-hosting.svc.cluster.local:8080/v1}
          tls_verify: false
          api_token: ${env.GRANITE_TOKEN:fake}
          #max_tokens: 12000
          #tls_verify: false
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      - provider_id: vllm-safety
        provider_type: remote::vllm
        config:
          url: ${env.SAFETY_MODEL:http://llama-guard-3-1b-predictor.llm-hosting.svc.cluster.local:8080/v1}
          max_tokens: ${env.VLLM_MAX_TOKENS:4096}
          api_token: ${env.VLLM_API_TOKEN:fake}
          tls_verify: ${env.VLLM_TLS_VERIFY:false}
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db
      eval:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          kvstore:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/meta_reference_eval.db
      datasetio:
      - provider_id: huggingface
        provider_type: remote::huggingface
        config:
          kvstore:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/huggingface_datasetio.db
      - provider_id: localfs
        provider_type: inline::localfs
        config:
          kvstore:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/localfs_datasetio.db
      vector_io:
      - provider_id: faiss
        provider_type: inline::faiss
        config:
          kvstore:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/faiss_store.db
      safety:
      - provider_id: llama-guard
        provider_type: inline::llama-guard
        config:
          excluded_categories: []
      scoring:
      - provider_id: basic
        provider_type: inline::basic
        config: {}
      - provider_id: llm-as-judge
        provider_type: inline::llm-as-judge
        config: {}
      - provider_id: braintrust
        provider_type: inline::braintrust
        config:
          openai_api_key: ${env.OPENAI_API_KEY:}
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: ${env.OTEL_SERVICE_NAME:llama-stack}
          sinks: ${env.TELEMETRY_SINKS:console, sqlite}
          otel_trace_endpoint: ${env.OTEL_TRACE_ENDPOINT:}
          sqlite_db_path: ${env.SQLITE_DB_PATH:~/.llama/distributions/remote-vllm/trace_store.db}
      tool_runtime:
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config: {}
          #api_key: ${env.TAVILY_API_KEY}
          #max_results: 3
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
        config: {}
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
    metadata_store:
      type: sqlite
      db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm}/registry.db
    models:
    - metadata: {}
      model_id: granite-8b
      provider_id: vllm
      provider_model_id: granite-8b
      model_type: llm
    - metadata:
        embedding_dimension: 384
      model_id: all-MiniLM-L6-v2
      provider_id: sentence-transformers
      model_type: embedding
    - metadata: {}
      model_id: ${env.SAFETY_MODEL:meta-llama/Llama-Guard-3-1B}
      #provider_model_id: ${env.SAFETY_MODEL:llama-guard-3-1b}
      provider_id: vllm-safety
      model_type: llm
    datasets: []
    vector_dbs: []
    scoring_fns: []
    benchmarks: []
    shields:
    - shield_id: ${env.SAFETY_MODEL:meta-llama/Llama-Guard-3-1B}
    tool_groups:
    # - provider_id: tavily-search
    #   toolgroup_id: builtin::websearch
    # - toolgroup_id: builtin::rag
    #   provider_id: rag-runtime
    - toolgroup_id: mcp::openshift
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://ocp-mcp-server:8000/sse
    # - toolgroup_id: mcp::slack
    #   provider_id: model-context-protocol
    #   mcp_endpoint:
    #     uri: http://slack-mcp-server:80/sse